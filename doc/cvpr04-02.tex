%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Synthetic Surrealism
%

%

% This is a simple LaTex sample document that gives a submission format
%   for IEEE PAMI-TC conference submissions.  Use at your own risk.
 
% Make two column format for LaTex 2e.
\documentclass{ieee}
\usepackage{times}

% Use following instead for LaTex 2.09 (may need some other mods as well).
% \documentstyle[times,twocolumn]{article}

\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{algorithm,algorithmic}
\usepackage{subfigure}

\pagestyle{empty}
                     
% Set dimensions of columns, gap between columns, and paragraph indent 

% \setlength{\textheight}{8.875in}
% \setlength{\textwidth}{6.875in}
% \setlength{\columnsep}{0.375in}
% \setlength{\topmargin}{.625in}
% \setlength{\headheight}{0in}
% \setlength{\headsep}{0in}
% \setlength{\parindent}{1pc}
% \setlength{\oddsidemargin}{-.1875in}  % Centers text.
% \setlength{\evensidemargin}{-.1875in}

% \setlength{\textheight}{8.875in}
% \setlength{\textwidth}{6.875in}
% \setlength{\columnsep}{0.125in}
% \setlength{\topmargin}{0in}
% \setlength{\headheight}{0in}
% \setlength{\headsep}{0in}
% \setlength{\parindent}{1pc}
% \setlength{\oddsidemargin}{-.1875in}  % Centers text.
% \setlength{\evensidemargin}{-.1875in}

% Add the period after section numbers.  Adjust spacing.
% \newcommand{\Section}[1]{\vspace{-8pt}\section{\hskip -1em.~~#1}\vspace{-3pt}}
% \newcommand{\SubSection}[1]{\vspace{-3pt}\subsection{\hskip -1em.~~#1}
%         \vspace{-3pt}}


\def\A{{\cal A}}
\def\B{{\cal B}}
\def\C{{\cal C}}
\def\D{{\cal D}}
\def\E{{\cal E}}
\def\F{{\cal F}}
\def\J{{\cal J}}
\def\L{{\cal L}}
\def\M{{\cal M}}
\def\P{{\cal P}}
\def\Q{{\cal Q}}
\def\S{{\cal S}}
\def\V{{\cal V}}
\def\W{{\cal W}}
\def\X{{\cal X}}
\def\Y{{\cal Y}}
\def\SR{{\mathbb{R}}}
\def\MT{{\mathfrak{T}}}
\def\ME{{\mathfrak{E}}}
\def\MP{{\mathfrak{P}}}
\def\a{\alpha}
\def\e{{\bf e}}

\begin{document}

% Don't want date printed
%\date{}

% Make title bold and 14 pt font (Latex default is non-bold, 16pt) 
% \title{\large \bf Approximation of Canonical Sets and Their Applications
%   to 2D View Simplification }

% \title{Approximation of Canonical Sets and Their Applications
%   to 2D View Simplification }
                    
%\author{}

 % For two authors (default example)
%  \author{\begin{tabular}[t]{c@{\extracolsep{8em}}c}
%  I. M. Anonymous  & M. Y. Coauthor \\
%   \\
%          My Department & Coauthor Department \\
%          My Institute & Coauthor Institute \\
%          City, STATE~~zipcode & City, STATE~~zipcode
%  \end{tabular}}
%
% \author{\begin{tabular}[t]{c@{\extracolsep{3em}}c@{\extracolsep{3em}}c}
% Trip Denton   & Jeff Abrahamson & Ali Shokoufandeh \\
%         \emph{Drexel University} &  \emph{Drexel University} & \emph{Drexel University} %\\
% \end{tabular}}



\title{Approximation of Canonical Sets and Their Applications
  to 2D View Simplification }

\author{Trip Denton   \\
Drexel University \\
% For a paper whose authors are all at the same institution, 
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'', 
% just like the second author.
\and Jeff Abrahamson \\
Drexel University \\
\and Ali Shokoufandeh \\
Drexel University \\
}

 

\maketitle

\thispagestyle{empty}

\begin{abstract}

Given a set of patterns and a similarity measure between them, we
  will present an optimization framework to approximate a small
  subset, known as a canonical set, whose members closely resemble
  the members of the original set. We will present a combinatorial
  formulation of the canonical set problem in terms of quadratic
  optimization integer programming, present a relaxation through
  semidefinite programming, and propose a bounded performance
  rounding procedure for its approximation solution in
  polynomial time.  Through a set of experiments we will investigate
  the application of canonical sets for computing a summary of views
  from a dense set of 2D views computed for a 3D object.  
\end{abstract}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction \& Background}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
Given a set of patterns $\P=\{p_1,...,p_n\}$ and a similarity function
$\S:\P\times\P\rightarrow \SR^{\ge 0}$, the {\em canonical set} for
$\P$ is a subset $\P'\subseteq \P$ that best characterizes the
elements of $\P'$ with respect to the similarity function $\S$. To
motivate the problem, let us consider the 3D object in
Figure~\ref{fig:motivation}(a). Assume we have also obtained a dense
set of 2D views $\P$ of this object over the viewing sphere, such as
in Figure ~\ref{fig:motivation}(b), which shows 68 silhouettes
obtained from this 3D object. Our goal is to identify a small set of
views that closely resembles the full set of views in $\P$.

\begin{figure}[ht]
        \centerline{
        \begin{tabular}{c}
        \epsfxsize=0.3\hsize \epsfbox{clock-2.eps} \\
        (a) \\
        \epsfxsize=0.42\textwidth \epsfbox{clocks.eps} \\
        (b)
        \end{tabular}}
        \caption{Motivation: Given the 3D model in (a) and a  set
          of views in (b), and similarity function among the views,
          identify a small subset of views that best characterizes the
          object. The boxed views represent the canonical set obtained
          through our algorithm.}
        \label{fig:motivation}
\end{figure}
           
           
% \begin{figure}[ht]
%         \begin{center}
%         \epsfxsize=0.47\textwidth
%         \epsffile{figs/clock-2.eps} \\
%         \epsffile{figs/clocks.eps}
%         \end{center}
%         \caption{Sample views of the 3D objects used in our experiments.}
% \label{fig:motivation}
% \end{figure}
                                        

In developing an approximation framework for computing canonical sets,
we will take the novel view expressed by Cyr and Kimia~\cite{Cyr2001}
that ``the shape similarity metric between object outlines endows the
viewing sphere with a metric which can be used to cluster views into
aspects, and to represent each aspect with a prototypical view.'' In
fact, our work on canonical sets of a general class of patterns is in
large part motivated by novel ideas that were introduced in the
context of aspect graph representations~\cite{KOE79} and their
relevance in identifying regions of ``equivalent views'' on the
viewing sphere.  Intuitively, in this graphical representation, each
vertex represents an ``aspect'' of the 3D object, i.e., a maximally
connected region on the viewing sphere. The edges of this graph will
correspond to visual transitions between two neighboring general
views. See Bowyer and Dyer~\cite{Bowyer90} for a survey on aspect
graphs and their applications. To reduce the complexity of generating
the aspects, Eggert {\em et al.}~\cite{EGG93} developed the notion of
a scale-space aspect graph, and Dickinson {\em et al.}~\cite{DIC89}
used a hierarchical aspect graph system based on a finite set of
primitives.  Weinshall and Werman~\cite{WEI97} studied the notions of
view stability and view likelihood to establish a theory which defines
the aspect graph.

In the broader context of pattern simplification, computing
``equivalent views'' for pattern class $\P$ is closely related to that
of the clustering problem. Namely, if the size $k$ of the canonical
set---the number of typical views---is more or less known, then one
can use a clustering algorithm to partition $\P$ into subsets
$\P_1,...,\P_k$, and then choose an element $c_i$ near the center of
each cluster as the typical element of $\P_i$ for each $1\le i\le k$.
This technique has, in fact, been used to define aspect graphs for
polyhedra~\cite{STE88,SHI93}, and solids of revolution~\cite{EGG90}.
Clearly, if the number of subsets $k$ in the partition of $\P$ is not
known, then defining a centroid $c_i$ based on the pair-wise
similarity function $\S$ will be extremely difficult.

But there are still several questions we would like to ask. For
example, what if we do not know the size of the canonical set? How do
we know if the clustering algorithm is, in fact, creating reasonable
partitions? Or what if we change our notion of resemblance to a more
complex predicate? For example, ``I would like the elements of $\P'$
to be as dissimilar to each other as possible and its size to be at
least $c_1$ but not more than $c_2$.'' Alternatively, what if we
change the definition of similarity between elements in
$\P\setminus\P'$, the complement of $\P'$, and $\P'$?  Unfortunately,
despite the simplicity and strength of such clustering methods, there
is a clear need for further work.

This work is an attempt to address some of the above questions.  We
will present an original direct formulation of the canonical set
problem as an integer programming optimization problem with a well
defined set of objectives and a clear set of constraints.  In view of
the intractability of the integer programming problem, we present an
original but natural reduction to a semidefinite programming problem
(SDP) (section~\ref{sec:quadratic}).

Since the seminal work of Goemans and Williamson~\cite{GW94} for the
{\sc Max-Cut} problem in graphs, SDP relaxations have proven useful in
obtaining improved approximation algorithms for several optimization
problems. See Goemans \cite{goemans97semidefinite} and Mahajan and
Ramesh~\cite{mahajan99derandomizing} for a survey of recent results
and applications of SDP.

In order to preserve flexibility in the formulation, we will use a
multi-objective formulation.  To be able to solve the problem we will
clearly need somehow to combine the objectives in a meaningful way. We
will present the details of this setup in
section~\ref{sec:combine}. For an overview of the theory behind
combining such objective functions, the reader is referred to
Miettinen~\cite{nonlin-multiobj-opt}.

Given a solution of the semidefinite program, we will present a
mechanism for constructing the canonical sets
(section~\ref{sec:rounding}).  The rounding scheme used in the
construction of canonical sets is based on multivariate normal
distribution rounding. See Mahajan and
Ramesh~\cite{nonlin-multiobj-opt} for an overview of rounding and its
derandomization.  Finally, we will carry out a set of experiments to
evaluate the quality of solutions with respect to exhaustive search in
the context of 2D view simplification (section~\ref{sec:exp}).
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
\label{sec:form}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\vspace{-0.1in}
In this section we define the notion of canonical set.  This
combinatoric problem is unfortunately intractable.  We therefore
present a series of steps (integer programming, quadratic programming,
semidefinite programming, and finally rounding) that allow us to
compute good approximate solutions in polynomial time.
\vspace{-0.1in}
\subsection{Definitions}
\vspace{-0.1in}
Given a set of patterns $\P=\{p_1,...,p_n\}$ and a similarity function
$\S:\P\times\P\rightarrow \SR^{\ge 0}$, we define a corresponding
edge-weighted graph $G=G(\P)$ to capture the similarity structure
among patterns in $\P$.  The vertex set $V$ consists of nodes
corresponding to patterns $p_1,...,p_n$, and for every pair of
patterns $p_i,p_j\in \P$, there is an edge $(p_i,p_j)\in E$ of weight
$\S(p_i,p_j)$ if $\S(p_i,p_j)\ge \sigma$ for a threshold $\sigma$.
Thus, if the similarity is not sufficient, one pattern can never
dominate the other.  In a view-based setting, for example, the absence
of an edge between two views implies that the views do not belong to
the same class.
  
  A set of vertices $V'\subseteq V$ is a {\em
  dominating set} for $\P$ if for every vertex $u\in V\setminus V'$,
there exists a node $v\in V'$, with $(u,v)\in E$.  Observe that the
dominating set is different from a vertex cover in that for every
vertex, either the vertex or one of its neighbors is in $V'$.  The
minimum dominating set problem is the problem of finding a dominating
set $V^*$ of minimum cardinality.

For  a cut $(V',V\setminus  V')$ of  $G$, let  $\delta(V')\subseteq E$
denote the set of cut edges with respect to $V'$, i.e., the edges with
one end-point in $V'$ and  the other end-point in $V\setminus V'$.  We
define the cut-weight with respect to $V'$ as: 
$$\S(V')=\sum_{(p,q)\in  \delta(V')}  \S(p,q).$$

The {\em  canonical set} for patterns $\P$ (equivalently, 
its graph  $G$), is the  minimum dominating set $V^*$  with maximum
cut-weight.  Intuitively,  such a set  is a simultaneous  solution to
the dominating set and the maximum cut problems in graph $G$.\\

Our goal is to formulate the problem as a multi-objective optimization 
problem.  To this end we need to define appropriate indicator variables, 
objective functions, and constraints.  First we introduce some notation.  
We  use $I$ to denote the $n\times n$ identity matrix and $\e$ the all-ones 
vector of order $n$.  We  use $d\in \SR^n$ to denote the vector of 
degrees for $G$, i.e.,  $d_v$ is the degree of vertex $v\in V$.  
Let $A\in\{0,1\}^{n\times n}$ denote the adjacency matrix of graph $G$ \\
$$A_{i,j}=\left \{
\begin{array}{ll}
1 & {\rm if}\ (p_i,p_j)\in E,\\
0 & {\rm otherwise.}
\end{array}
\right.
$$ 
Let $\W\in\SR^{n\times n}$ denote the edge weight matrix of graph $G$
$$\W_{i,j}=\left \{
\begin{array}{ll}
\S(p_i,p_j) & {\rm if}\ (p_i,p_j)\in E,\\
0 & {\rm otherwise.}
\end{array}
\right.
$$
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integer Programming Formulation}
\label{sec:integer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
In this section, we present our formulation of the canonical set
problem as a multi-objective integer optimization problem.  For each
pattern $p_i$, $1\le i\le n$, we introduce a binary indicator variable
$y_i$.  The variable $y_i$ can have a value of $+1$ or $-1$,
indicating whether the corresponding pattern belongs to $V^*$ or
$V\setminus V^*$, respectively.  Let $y \in \{-1,+1\}^n$ be the vector
$[y_1,\ldots,y_n]^t$.

The first property a canonical  set needs to satisfy is the
dominating set property. That is, for every pattern $p_i$,
\begin{equation}
\label{eq:eq1}
 (1+y_i)+\sum_{j=1}^n A_{i,j}(1+y_j)\ge 2.
%       \frac{1}{2} \sum_{i,j}w_{ij} (1-y_iy_j) 
\end{equation}
This means that every pattern $p_i$ is either in the dominating set, or 
has at least one of its immediate neighbors in the dominating set.
%% In 
%% matrix form this can be written as: 
%% \begin{equation}
%% \label{eq:eq2}
%% (A+I)y\ge \e-d.
%% \end{equation}
To guarantee minimum cardinality of the set $V^*$, we formulate the constraint
\begin{equation}
\label{eq:eq3}
{\rm Minimize}\ {\frac{1}{2}}\sum_{i=1}^n (1+y_i) {={\frac{1}{2}}(n+\e^ty).}
\end{equation}
or, equivalently, to maximize the cardinality of $V\setminus V^*$ 
\begin{equation}
\label{eq:eq4}
{\rm Maximize}\ {{\frac{1}{2}}(n-\e^ty).}
\end{equation}
Observe that the indicator variable $y$ corresponding to a subset $V^*$ 
satisfies $1-y_iy_j=2$ if $(p_i,p_j)\in\delta(V^*)$ and 0 otherwise.

Next, the maximization of similarity (cut weight) between patterns
in $V^*$ and those in $V\setminus V^*$ can be formulated as
\begin{equation}
\label{eq:eq5}
{\rm Maximize}\ {\frac{1}{4}}\sum_{i=1}^n\sum_{j=1}^n \W_{ij}(1-y_iy_j)
\end{equation}
Finally, putting (\ref{eq:eq1}), (\ref{eq:eq4}), and (\ref{eq:eq5}) together,
we have our integer programming formulation:
\begin{eqnarray*} 
{\rm Maximize}       && {\displaystyle \frac{1}{2}(n-\e^ty)} \\
{\rm Maximize}       && {\displaystyle \frac{1}{4}\sum_{i=1}^n\sum_{j=1}^n \W_{ij}(1-y_iy_j)} \\
{\rm Subject\ to}   && {\displaystyle  (1+y_i)+\sum_{j=1}^n A_{i,j}(1+y_j)\ge 2},   \nonumber\\
  && {\displaystyle y \in \{-1,+1\}^n}
\end{eqnarray*}

\vspace{-0.1in}                                                                
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quadratic Integer Programming} 
\label{sec:quadratic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
The integer programming formulation described in
section~\ref{sec:integer} is unfortunately known to be
NP-hard~\cite{Garey79}.  In order to develop an approximation
solution, our approach is to give a reformulation as a quadratic
integer programming problem and then to use vector labeling and a
semidefinite programming (SDP) relaxation to obtain an approximate
solution in polynomial time.

In its current form, the problem is heterogeneous, because equation
(\ref{eq:eq5}) is quadratic and the others are linear.  We therefore
homogenize everything to a quadratic form to prepare for the SDP
relaxation.
 
First we introduce a \emph{set indicator} variable
$y_{n+1}\in\{-1,+1\}$, that is, $p_i\in V^*$, $1\le i\le n$, if and only if
$y_i=y_{n+1}$.  This gives us
$$p_i \in V^* \Leftrightarrow \frac{1+y_iy_{n+1}}{2}=1,$$
\begin{equation}
\label{eq:eq6}
p_i \notin V^* \Leftrightarrow \frac{1-y_iy_{n+1}}{2}=1.
\end{equation}
We can reformulate the cardinality objective function (\ref{eq:eq3}) as
\begin{equation}
\label{eq:eq7}
{\rm Minimize}\ {\frac{1}{2}}\sum_{i=1}^n (1+y_iy_{n+1}) 
\end{equation}
or equivalently as               
\begin{equation}
\label{eq:eq8}
{\rm Maximize}\ {\frac{1}{2}}\sum_{i=1}^n (1-y_iy_{n+1}).
\end{equation}
Similarly the dominating set constraint (\ref{eq:eq1}) can be rewritten as
\begin{equation}
\label{eq:eq9}
 (1+y_iy_{n+1})+\sum_{j=1}^n A_{i,j}(1+y_jy_{n+1})\ge 2.
\end{equation}
                                          
                                          
Combining~(\ref{eq:eq5}),~(\ref{eq:eq8}), and~(\ref{eq:eq9}) we 
arrive at the following quadratic formulation of the canonical set
problem:

\begin{eqnarray*} 
{\textbf{(QIP):}}\\
{\rm Maximize} && {\displaystyle \sum_{i=1}^n \frac{1-y_iy_{n+1}}{2}} \\
{\rm Maximize} && {\displaystyle{\frac{1}{4}}\sum_{i=1}^n\sum_{j=1}^n \W_{ij}(1-y_iy_j)} \\
{\rm Subject\ to} && {\displaystyle (1+y_iy_{n+1})}\\
                  && {\displaystyle  +\sum_{j=1}^n a_{ij}({1+y_jy_{n+1}}) \ge 2}, \ \forall\ 1\le i\le n \nonumber\\
 &&  y_i\in\{-1,+1\},\ \forall\  1\le i\le n+1
\end{eqnarray*}

\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
\subsection{Semidefinite Programming Relaxation} 
\label{sec:semidefinite}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
Since the quadratic formulation is still intractable, we use vector
labeling to reformulate the problem as a semidefinite program.  In
canonical form, a semidefinite programming problem can be
characterized as
$$
\begin{array}{lll}
{\rm Maximize}       & \C \bullet \X         &  \\
{\rm Subject\ to\ }   & \D_i \bullet \X=d_i,  & \forall i=1,\ldots,n  \\
                     & \X \succeq 0,         &
\end{array}
$$
where matrices $\C,\ \D_i$, and $\X$ are all real symmetric
matrices, and $\A \bullet \B$ denotes the Frobenius inner product of
matrices $\A$ and $\B$, i.e. $\A \bullet \B= \mathbf{Trace}(\A^t \B)$.

As is conventional in SDP relaxations of quadratic integer programming
problems, we introduce a vector for each indicator
variable~\cite{vandenberghe96semidefinite,goemans97semidefinite}.  That is, we replace each
$y_i$ with a vector $x_i \in S_{n+1}$, where $0 \leq i \leq n$ and
$S_{n+1}$ is the unit sphere in $\SR^{n+1}$. First we address equation
(\ref{eq:eq8}), which maximizes the number of vertices that are
\emph{not} in $V^*$. Replacing each
variable $y_i$ with the vector $x_i$, we obtain
\begin{equation}
\label{eq:eq10}
{\rm Maximize}\ {\frac{1}{2}}\sum_{i=1}^n (1-x_i^tx_{n+1}).
\end{equation}
Define the matrix $\V=[x_1,...,x_n,x_{n+1}]$ to be the matrix obtained by
concatenating the column vectors $x_i$, $0\le i\le n$, and define
$\X=\V^{\,t}\V$. Clearly,  $\X$ is a semidefinite matrix, since
$x^t \X x =x^t \V^{\,t}\V x=||\V x||_2^2,$ $\ \forall x\in \SR^{n+1}.$
Observe that
$\sum_{i=1}^n x_i^tx_{n+1}= \E \bullet \X/2,$
where $ \E= \left [ 
\begin{array}{cc}
\tilde{\bf 0} & \e \\
\e^t & 0 
\end{array} 
\right]$ and $\tilde{\bf 0}$ is an $n\times n$ matrix of all zeroes. 
If we let $\J$ be an $(n+1)\times(n+1)$ all ones matrix, then we can
rewrite the relaxed cardinality objective of (\ref{eq:eq10}) as
\begin{equation}
\label{eq:eq11}
{\rm Maximize }\ \frac{1}{2}\E \bullet(\J-\X)
\end{equation}
or, equivalently, as 
\begin{equation}
\label{eq:eq13}
{\rm Maximize }\ {\frac{n}{2}-\frac{1}{2}\E \bullet \X}.
\end{equation}
This, in turn, can be stated in minimization form as
\begin{equation}
\label{eq:eq14}
{\rm Minimize }\ {\frac{1}{2}\E \bullet \X}.
\end{equation}
Similarly, we rewrite the similarity objective in (\ref{eq:eq5}) as
\begin{displaymath}
  \sum_{i=1}^n\sum_{j=1}^n \W_{ij}(1-y_iy_j)=~\widetilde{\W} \bullet (\J-\X),
\end{displaymath}
where $
~\widetilde{\W}= \left[
\begin{array}{cc}
\W & \vec{\bf 0} \\
\vec{\bf 0}^t & 0  
\end{array}
\right]
$ and $\vec{\bf 0}$ is an all zeroes vector in $\SR^n$. 
Consequently, the second objective function becomes
\[{\rm Maximize}\ \frac{1}{4}~\widetilde{\W} \bullet \J - \frac{1}{4}~\widetilde{\W} \bullet\X,\]
or, equivalently,
\begin{equation}
\label{eq:eq15}
{\rm Minimize }\ \frac{1}{4}~\widetilde{\W} \bullet\X.
\end{equation}
Lastly, the constraints for the dominating set property in (\ref{eq:eq9})
can be relaxed (for each $1\le i\le n$) as
\[ (1+x_i^tx_{n+1})+\sum_{j=1}^n A_{i,j}(1+x_j^tx_{n+1})\ge 2\mbox{,}\]
which is the same as
$x_i^tx_{n+1}+\sum_{j=1}^n A_{i,j}(x_j^t)\ge 1-d_i$,
where $d_i$ is the degree of vertex $i$. Defining the coefficient matrix 
$\D^i $ for every constraint $i=1,\ldots,n$ as
$$
\D^i= \left[
\begin{array}{ccccccc}
 0      &      0&  \ldots &  0   &  0       &\ldots &A_{i,1}   \\
 0      &      0&  \ldots &  0   &  0       &\ldots &A_{i,2}   \\
 \vdots & \vdots&  \ddots &\vdots&\vdots    &\ddots &\vdots   \\
 0      &      0&  \ldots &0     &0         &\ldots &1        \\
 0      &      0&  \ldots &0     &0         &\ldots &A_{i,i+1}\\
 \vdots & \vdots&  \ddots &\vdots&\vdots    &\ddots &\vdots   \\
% 0      &      0&  \ldots &0     &0           &\ldots &A_{i,n}   \\
 A_{i,1}& A_{i,2}&  \ldots &  1   &A_{i,i+1} &\ldots & 2(d_i-1) \\
\end{array}
\right]
$$
and using  $x_{n+1}^2=1$, we get the following set of
constraints in our relaxed SDP formulation
\begin{equation}
\label{eq:eq16}
\D^i \bullet \X \ge 0,\ \forall i=1, \ldots, n.
\end{equation}
Observe that the integrality constraints $y_i\in\{-1,+1\}$ are
replaced by $diag(\X)=\e$, so the diagonal of $\X$ is all ones.
Putting together (\ref{eq:eq14}), (\ref{eq:eq15}), and (\ref{eq:eq16})
we get our semidefinite relaxation:
\begin{eqnarray*}
{\textbf{(SDP):}}\\
{\rm Minimize }&& {\displaystyle \frac{1}{2}\E \bullet \X}                             \\
{\rm Minimize }&& {\displaystyle \frac{1}{4}~\widetilde{\W} \bullet\X}                \\
{\rm Subject\ to} && {\displaystyle \D^i \bullet \X \ge 0}, \  \forall i=1, \ldots, n,  \\
                    && {\displaystyle  diag(\X )=\e},          \\
                    && {\displaystyle  \X \succeq 0}.          
\end{eqnarray*}
Finally, note that both objectives in our SDP formulation are convex
functions. This is due to the fact that Jacobians of both $\E \bullet
\X$ and $~\widetilde{\W} \bullet\X$ are non-negative semidefinite
matrices.
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Combining Objectives}
\label{sec:combine}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
Our SDP formulation of the canonical set problem is a multi-objective
optimization problem that seeks to find the optimal solution for a set
of objective functions.  The general set up of such optimization
problems is as
follows~\cite{multicriteria-optimization,nonlin-multiobj-opt}:
$$
\begin{array}{ll}
{\rm Minimize }& \F(\X)=\{\displaystyle f_1(\X), f_2(\X), \ldots, f_m(\X))\} \\
{\rm Subject\ to} & {\displaystyle \X \in \Gamma},
\end{array}
$$
where $\F(\X)=\{f_1(\X),\ldots, f_m(\X)\}$ is the set of objective
functions and $\Gamma$ is the feasible set. 

Observe that the objective
functions of the SDP formulation for the canonical set problem are somewhat
competing, in that a solution that produces a canonical set of minimum
cardinality does not necessarily maximize the similarity objective.
In such a set up, a trade-off optimality condition known as {\em
  Pareto} optimality is used~\cite{multicriteria-optimization}.
Specifically, a solution $\X^*$ is called Pareto optimal if there is
no $\X \in \Gamma$ such that $\F(\X) \le \F(\X^*)$; that is, $\X^*$ is
lexicographically optimal compared to any sub-optimal solution $\X$.
Observe that such a solution is not necessarily unique. The set of all
Pareto optimal solutions $\X^* \in \Gamma$ is denoted by $\Gamma_{Par}$,
the {\em Pareto set}. The Pareto set can be thought of as the boundary
of the image of the feasible set, and a Pareto optimal solution is a
point on the boundary.

If in a multi-objective optimization problem, the functions
$f_i(\X)$, $1\le i\le m$, are all convex functions, then the optimal
solution $\X^*$ of the following single-objective problem  belongs to
the Pareto set of problem $\F(\X)$~\cite{nonlin-multiobj-opt}:
\begin{equation}
\label{eq:eq17}
\begin{array}{ll}
{\rm Minimize }& {\displaystyle \sum_{i=1}^m \alpha_i f_i(\X)} \\
{\rm Subject\ to} & {\displaystyle \X \in \Gamma}, \\
{\rm where} & {\displaystyle \alpha_i \ge 0 \quad \forall\ 1\le i\le m}, \\
                   & {\displaystyle \sum_{i=1}^m \alpha_i=1.}
\end{array}
\end{equation}
In fact, our first approach for combining the cardinality and
similarity objectives of the multi-objective SDP formulation is based
on using such a convex combination of the two objective functions.
Specifically, we let $\C_1=\alpha\E / 2$ and define the normalized
weight matrix $\C_2=(1-\alpha)\widetilde\W /\left (4\sum_{i,j}w_{ij}\right )$
for a convexity
parameter $\alpha\in [0,1]$. The normalization of the weight function has
the effect of not allowing any combination of cut edges to outweigh a
single vertex. The combined objective function then  satisfies
$$
\begin{array}{ll}
{\C_1 \bullet \X+\C_2 \bullet \X}&= \mathbf{Trace}(\C_1^t \X) + \mathbf{Trace}(\C_2^t \X) \\
&= \mathbf{Trace}((\C_1  + \C_2)^t \X).
\end{array}
$$
Letting $\C(\alpha)=\C_1+\C_2$, our multi-objective SDP formulation
is
$$
\begin{array}{ll}
{\textbf{(CoSDP):}}\\
{\rm Minimize}       & \mathbf{Trace}(\C^t \X)  \\
{\rm Subject\ to} & {\displaystyle \D^i \bullet \X \ge 0,\ \forall i=1, \ldots, n }  \\
                    & {\displaystyle  diag(\X )=\e},          \\
                    & {\displaystyle  \X \succeq 0}.
\end{array}
$$
Finally, for every $\alpha\in[0,1]$ the optimal solution
$\X^*=\X^*(\alpha)$ can be computed in polynomial time using the
algorithm \cite{ali-95} (for details on a SDP solver the reader is
referred to~\cite{toh99sdpt}).

\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Rounding Scheme}
\label{sec:rounding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
The final stage in our approximation algorithm, once we have solved
the semidefinite program, is to construct a feasible integer solution
to the canonical set problem.

This process, also known as rounding, identifies the set of values for
indicator variables $y_1,...,y_n$ and the set indicator variable
$y_{n+1}$.  In our experiments, we have used a rounding scheme based
on Cholesky decomposition and a multivariate normal hyperplane method
that can be effectively derandomized.
(See~\cite{mahajan99derandomizing} for details.)

Let $\X^*=\X^*(\alpha)$ denote the optimal solution of the
\textbf{(CoSDP)} for a given $\alpha\in[0,1]$.  Since $\X^*$ is a
symmetric positive semidefinite matrix, using Cholesky decomposition
it can be represented as $\X^*=\V^t\V$ ~\cite{Golub96}. This provides
us with $n+1$ vectors for the relaxed canonical set problem.
Specifically, column $x_i$, $1\le i\le n$, of $\V$ forms the vector
associated with vertex $v_i\in G$ in the optimal SDP relaxation of the
canonical set problem, and column $x_{n+1}$ corresponds to the set
indicator variable.
 
Finally, we pick a random vector $\rho\in\SR^{n+1}$ with multivariate
normal distribution, with $0$ mean, and with covariance $\X^*$, to
generate the indicator variables $y_i$, $i\le 1\le n+1$ and the
canonical set $V^*$ as $ y_i=\textbf{sign}\left (\rho^tx_i\right ) $
and $ V^*=\left\{v_i\,|\,y_i=y_{n+1}, 1\le i\le n \right\}.  $

Note that as a result of this rounding step, some of the constraints
of \textbf{(QIP)} might be violated. Namely, for some vertex $v$,
neither $v$ nor any of its neighbors may belong to set $V^*$. Let
$\widetilde{V}$ denote the set of all such vertices, and
$\ell=|\widetilde{V}|$. For every $v\in \widetilde{V}$ we define its
potential $\phi(v)$ as the sum of edge-weights of neighbors of $v$
normalized by the size of its neighborhood, i.e,
$\phi(v)={\displaystyle \left (\sum_{u\in {\rm adj}(v)}\W_{v,u}\right
  )\left /d_v\right .}$, where ${\rm adj}(v)$ is the neighborhood of
$v$ and $d_v$ is the degree of $v$.  Next we sort the potential values
of all vertices in $\widetilde{V}$, obtaining a sequence $\phi(v_1)\ge
\phi(v_2)\ge ...\ge \phi(v_\ell)$.  We iteratively add vertex $v_1$ to
$V^*$, remove $v_1$ and all its neighbors in ${\rm adj}(v)$ from
$\widetilde{V}$, and recompute the potential of all remaining vertices
in $\widetilde{V}$.  This greedy process tries to add vertices into
$V^*$ in increasing order of potential contribution to both
cardinality and similarity and can be repeated at most $\ell$ times.
It is worth mentioning that such a greedy algorithm can be started
from the beginning to try to find an approximation solution with $V^*$
initially empty.  Experiments comparing the greedy algorithm to our
SDP formulation confirm that SDP produces better results.

Alternatively, we can run the rounding algorithm multiple times and
keep the best result.  For details of the rounding scheme and quality
of the post-rounding solution the reader is referred
to~\cite{VaziraniApproxAlg2003}.
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final Algorithm}
\label{sec:final}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
Our algorithm for computing the canonical set for a given class of
patterns $\P=\{p_1,p_2,...,p_n\}$ under similarity function $\S$ is a
combination of the previous procedures. Specifically, given the pair
$(\P,\S)$, we form the weighted graph $G(\P)$, and form the
semidefinite programming formulation, and the combined objective
functions. We next solve the resulting semidefinite optimization
program to obtain the solution $\X^*$.  To construct the
vector-coloring of nodes in $G(\P)$, we compute the Cholesky
decomposition of $\X^*$. Finally, to form the canonical set $\V^*$, we
use the rounding scheme and fix-up steps.  We summarize our approach
in Algorithm~\ref{alg:canset}.

\begin{algorithm}
\begin{algorithmic}[1]

\STATE Construct the graph $G(\P)$ according to
section~\ref{sec:form}.

\STATE Form the semidefinite program with combined objective
\textbf{(CoSDP)} according to
sections~\ref{sec:quadratic} and~\ref{sec:combine} .

\STATE Solve \textbf{(CoSDP)} using the algorithm in
~\cite{toh99sdpt}, obtaining PSD matrix $\X^*$.

\STATE Compute the Cholesky decomposition $X^*=\V^t\V$. 

\STATE Construct the indicator variables $y_1,...,y_n,y_{n+1}$ and
form the set set $V^*$ according to section~\ref{sec:rounding}.
\end{algorithmic}
\caption{Approximation of Canonical Set}
\label{alg:canset}
\end{algorithm}

Steps one, two, and five of algorithm~\ref{alg:canset} each have
running time $O(n^2)$.  Using Alizadeh's adaption of the
interior-point method~\cite{ali-95}, step three can be implemented in
time $O(\sqrt n (\log L + \log 1/\epsilon) n^3 )$ where $\log L$ is
the size of the coefficient matrix $\widetilde{W}$ and $\epsilon$ is
the error on the quality of the SDP solution.  Finally, step four
requires $O(n^3)$.
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:exp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
In this section we present an overview of experiments we have
performed to date to evaluate the optimization framework for computing
canonical sets. Each pattern class in our experiments corresponds to a
set of 2D views acquired from a 3D object.  Specifically, we use 9
objects, each representing a single pattern class. For each object we
have as many as 180 2D views acquired along a great circle of the
viewing sphere, giving a total of 1620 views.  A representative view
of each object is shown in Figure~\ref{fig:db}.
\begin{figure}[ht]
        \begin{center}
        \epsfxsize=0.47\textwidth
        \epsffile{db9c.eps}
        \end{center}
        \caption{Sample views of the 3D objects used in our experiments.}
\label{fig:db}
\end{figure}

Next we use a distance function to measure the similarity among the
patterns that constitute each pattern class.  In our previous
work~\cite{shokou_cvpr_2003}, we have developed an algorithm for
computing the many-to-many matching, as well as similarity measure, of
2D views represented as silhouettes.  For a given view an object's
silhouette is first represented by an undirected, rooted, weighted
graph, in which nodes represent {\em shocks} \cite{SID99} (or,
equivalently, skeleton points) and edges connect adjacent shock
points. The shock graphs will, in turn, be represented in terms of
shock trees using a minimum spanning tree of the weighted shock graph.
(For details on the construction of this trees,
see~\cite{shokou_cvpr_2003}.)

An illustration of this representation
is given in Figure~\ref{fig:medial-axis-tree}.  The left portion shows
the initial silhouette and its shock points (skeleton).  The right
portion depicts the constructed shock tree.  Darker, heavier nodes
correspond to fragments whose average radii are larger.
\begin{figure}[ht]
        \centerline{
        \begin{tabular}{c c}
        \epsfxsize=0.25\hsize \epsfbox{teapot0015-skeleton.ps} &
        \epsfxsize=0.25\hsize \epsfbox{teapot0015-graph.ps}
        \end{tabular}}
%        \begin{tabular}{c}
%        \epsfxsize=1in \epsfbox{figs/teapot0015-skeleton.ps}\\
%        \epsfxsize=1in \epsfbox{figs/teapot0015-graph.ps}}
%        \end{tabular}}
        \caption{Left: the silhouette and its medial axis. Right: the
          medial axis  tree constructed from the  medial axis.  Darker
          nodes reflect larger radii.}
        \label{fig:medial-axis-tree}
\end{figure}

Figure~\ref{fig:mtm} illustrates the many-to-many correspondences
provided by the algorithm in ~\cite{shokou_cvpr_2003} for two adjacent
views (30$^\circ$ and 40$^\circ$) of the {\sc Teapot}.  Corresponding
clusters (many-to-many mappings) have been shaded with the same color
(the extraneous branch in the left view was not matched in the right
view), and the algorithm provides an overall measure of similarity
between the two views as well.

\begin{figure}[ht]
        \centerline{
        \begin{tabular}{c}
        \\
        \epsfxsize=0.50\hsize \epsfbox{corres.eps}
        \end{tabular}}

        \caption{Illustration of the correspondences produced by the 
          matching algorithm to compute the similarity measure between
          objects.}
        \label{fig:mtm}
\end{figure}

For the experiments, we compute the shock tree representation of every
silhouette, and used the many-to-many matching algorithm
of~\cite{shokou_cvpr_2003} to compute the distance values among 2D
silhouettes corresponding to each 3D object (pattern class).  The
outcome of this procedure is distance matrices. To form similarity
matrices, and thus the graph $G(\P)$, we set to zero all elements
whose distance is greater than the mean distance and use a similarity
value of $e^{-d}$ for distance $d$ less than the mean.

Choosing the convexity parameter $\alpha$ is a critical step in
generating the combined objective function (see
section~\ref{sec:combine}). To this end we performed a set of
experiments in which we measured the value of~\textbf{(CoSDP)} as the
value of $\alpha$ was varied in the interval $[0,1]$. We have observed
that a balanced trade-off between the cardinality of the canonical set and the
similarity objectives, i.e., $\alpha=0.50$, produces the most consistent
results.
%
% $\alpha\in[0.65,0.69]$, with small tilt
% toward similarity objective, provides locally maximum value for
% the combined objective~\textbf{(CoSDP)}. T
%
This by no means is a general
assertion, and  certainly needs a closer investigation with respect
to the similarity function $\S$. 

%%  \begin{figure}[ht]
%%  \begin{center}
%%  \epsfsize=0.7\hsize
%%  \epsffile{figs/chart1.eps}
%%  \end{center}
%%  \caption{The change in combined objective~\textbf{(CoSDP)} as a fuction 
%%           convexity parametr $\alpha$ for pattern class corresponding to {\sc Clock}.}
%%  \label{fig:chart1}
%%  \end{figure}
 
Next we created 9 smaller pattern classes from our original classes,
with about 20 views per object. For each pattern class we searched the
space of all possible subsets and identified the smallest canonical
set whose value for the similarity objective is maximum. Next we used
algorithm~\ref{alg:canset} to compute an approximation solution
for~\textbf{(CoSDP)} with balanced combined objectives.
Figure~\ref{fig:chart2} represents the performance ratio between the
similarity objective of the canonical set obtained from
algorithm~\ref{alg:canset} with that of the exhaustive search. In
these cases results of algorithm~\ref{alg:canset} were within a factor
of 0.84 of exhaustive search. Figure~\ref{fig:cansets} illustrates the
sample views of several pattern classes and the canonical sets
computed for each class (outlined in blue).


  \begin{figure}[ht]
          \begin{center}
            \epsfsize=0.8\hsize \epsffile{chart2.eps}
          \end{center}
          \caption{Comparing the similarity objective for canonical 
            sets obtained from Algorithm~\ref{alg:canset} and
            for exhaustive search.}
  \label{fig:chart2}
  \end{figure}

\begin{figure}[!ht]
  \centerline{
        \begin{tabular}{|c|}
        \hline
        \epsfxsize=0.35\textwidth \epsfbox{blue_PORSHE_17.dist.imgs.eps} \\
        (a) \textsc{Porshe} \\
        \hline
        \epsfxsize=0.35\textwidth \epsfbox{blue_TEAPOT_18.dist.imgs.eps} \\
        (b) \textsc{Teapot} \\
        \hline
        \epsfxsize=0.35\textwidth \epsfbox{blue_CUP_18.dist.imgs.eps}\\
        (c) \textsc{Cup} \\
        \hline
        \epsfxsize=0.35\textwidth \epsfbox{blue_CHAIR_18.dist.imgs.eps}\\
         (d) \textsc{Chair} \\
%%         \hline
%%         \epsfxsize=0.4\hsize \epsfbox{images/blue_PHONE_18.dist.imgs.eps} &
%%         \epsfxsize=0.4\hsize \epsfbox{images/blue_CAMERA_18.dist.imgs.eps}\\
%%          (e) \textsc{Phone} & (f) \textsc{Camera} \\
        \hline        
        \end{tabular}}
        \caption{Canonical sets (blue rectangles) for four sets of views.}
        \label{fig:cansets}
\end{figure}

Examining figure~\ref{fig:cansets} subjectively, we see that the
results for \textsc{Porshe} and \textsc{Cup} are meaningful. For
example, in the case of \textsc{Porshe}, the canonical set has a
partial profile and partial frontal view. Similarly, \textsc{Cup}
shows a view with handle and one without.

In the case of \textsc{Teapot}, on the other hand, visually one view
appears to be redundant. Upon close investigation, we realize we are
not currently minimizing the similarity between members of the
canonical set.

In the case of \textsc{Chair}, we would have expected the bottom right
chosen view to be instead the full frontal view to its left. The
algorithm did not produce as good results in this experiment.
Unfortunately, at this point we do not have a complete understanding
of the correspondence between the optimal canonical set and our
psychophysical perception. We suspect our distance function is not
properly modeling visual distance.
Running the algorithm on a series of 90 views of the chair took 41
seconds (wall time) on a Pentium III running at 1133 MHz.
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Future Work}
\label{sec:conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
We have developed an approximation framework for the canonical set
problem based on SDP relaxation of an integer programming formulation.
Through a series of experiments we evaluated this framework in the
context of 2D view simplification of 3D objects. Our results compared
favorably to exhaustive search.

Our definition of canonical set was limited to a single pattern class.
In reality we must always consider the application domain. For
example, consider the problem of image indexing.
Given multiple pattern classes, we may regard them as two
classes: the class of interest and all others. We might ask for a
canonical set of the class of interest, maximally similar to members
of its own class while maximally dissimilar to patterns outside it.

Specifically, let $\P=\{p_1,...,p_n\}$ denote the class of interest
and $\Q=\{q_1,...,q_m\}$ the set of patterns outside $\P$. We might ask
for the canonical set $\P^*$ for $\P$ such that each view in $\P^*$ is
maximally dissimilar from the views in $\Q$. This corresponds to using
the following objective function:
\begin{displaymath}
  \mbox{Minimize\ }
  \frac 1 4 \sum_{i=1}^n \sum_{j=1}^m \left(1-y_iy_{n+1}\right)\S(p_i,q_j).
\end{displaymath}

We also expect that canonical sets may prove useful in clustering. The
canonical set itself may provide proposed centroids of clusters, with
a minimum distance classifier completing the task. This may prove
useful for the segmentation problem.

In addition to these significant areas of further research, several
questions about canonical sets remain open.  We would like to
establish performance guarantees for the canonical set framework.  In
addition, we would like to better understand the meaning of an optimal
canonical set for our 2D view application, as well as to explore the
effects of the similarity computation on the psychophysical quality of
the canonical set.

Finally, we would like to explore the relationship between the
sparsity of the similarity matrix and the cardinality of the canonical
sets found.
\vspace{-0.1in}
\section*{Acknowledgments}
\vspace{-0.1in} The authors would like to thank Sven Dickenson
(University of Toronto), M.  Fatih Demirci (Drexel University) and
Yakov Keselman (DePaul CTI) for their insightful comments.  Funded in
part by grants form National Science Foundation (NSF/EIA 02-05178) and
Office of Naval Research (ONR-N000140410363).


\bibliographystyle{ieee}
\bibliography{tdenton}

\end{document}
